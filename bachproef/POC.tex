%%=============================================================================
%% Conclusie
%%=============================================================================

\chapter{Proof of Concept}
\label{ch:Proof of Concept}

In dit hoofdstuk wordt de opbouw van de Patroni cluster uitgelegd. Hierin zal worden gekeken hoe de cluster voldoet aan de functionele requirements, namelijk replicatie, failover en monitoring.
Als proof of concept is gekozen voor Patroni omdat uit de requirementanalyse deze oplossing het sterkst voldeed aan de requirements.

\section{\IfLanguageName{dutch}{Omgeving}{Environment}}
\label{sec:Omgeving}
De opzet van deze proof of concept zal gebeuren in een lokale Linux-omgeving, namelijk Ubuntu 21.04.
De cluster zal bestaan uit drie virtuele machines die zullen draaien op Ubuntu Xenial 16.04. De eerste virtuele machine (pgServer) is een servernode waarop Consul zal draaien, samen met pgBouncer en een HAProxy. De andere twee virtuele machines (pgNode1 en pgNode2) zullen PostgreSQL, Patroni en een Consul Agent draaien.

De cluster zal dus zo geconfigueerd worden dat er één primary node is met een standby node die asynchrone streaming replicatie verricht.

\begin{table}
    \centering
    \begin{tabular}{ |p{6cm}||p{6cm}|  }
        \hline
        \multicolumn{2}{|c|}{Ip-adressen} \\
        \hline
        pgServer & 172.16.0.11 \\
        \hline
        pgNode1 & 172.16.0.22 \\
        \hline
        pgNode2 & 172.16.0.33 \\
        \hline
    \end{tabular}
    \caption{Ip-adressen Cluster}
    \label{table:Ip-adressen Cluster}
\end{table}

\section{\IfLanguageName{dutch}{Prerequisites}{Prerequisites}}
\label{sec:Prerequisites}

\subsection{\IfLanguageName{dutch}{Vagrant}{Vagrant}}
\label{subsec:Vagrant}
Bij de opzet van cluster zal gebruik worden gemaakt van Vagrant. Vagrant is een tool voor het bouwen en beheren van virtuele machine-omgevingen~\autocite{Kalow2020}.

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=Java,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

In onderstaande code snippet is de code voor de Vagrantfile zichtbaar. Hierin wordt bepaald welke servers aangemaakt worden en welke opties zij bezitten. Zo zal elke node een RAM-geheugen hebben van 1 GB (1024 MB). Ook het ip-adres zal hierin worden toegewezen per node. Er wordt ook een bestand meegegeven dat zal worden uitgevoerd in de server. Dit bestand bevat de opbouw van de cluster, zoals het installeren van Consul, het configureren van Patroni op de nodes, en de setup van HAProxy en pgBouncer.

Bij de drie virtuele machines wordt telkens verwezen naar een Linux bash-script, namelijk “postgresql-cluster-setup.sh”. Dit is het bestand waarin wordt gekeken met welke virtuele machine er gewerkt wordt, zodat, op basis daarvan, de juiste packages geïnstalleerd kunnen worden. Zo zal bijvoorbeeld bij pgNode1 Python, Consul Agent, Patroni en PostgreSQL geïnstalleerd worden als packages.

\begin{lstlisting}
# -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    config.vm.box = "ubuntu/xenial64"
    config.vm.provider "virtualbox" do |v|
        v.customize ["modifyvm", :id, "--memory", 1024]
        v.gui = true
    end

    config.vm.define :pgServer do |pgServer_config|
        pgServer_config.vm.hostname = 'pgServer'
        pgServer_config.vm.network :private_network, ip: "172.16.0.11"
        pgServer_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
    config.vm.define :pgNode1, primary: true do |pgNode1_config|
        pgNode1_config.vm.hostname = 'pgNode1'
        pgNode1_config.vm.network :private_network, ip: "172.16.0.22"
        pgNode1_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
    config.vm.define :pgNode2 do |pgNode2_config|
        pgNode2_config.vm.hostname = 'pgNode2'
        pgNode2_config.vm.network :private_network, ip: "172.16.0.33"
        pgNode2_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
end
\end{lstlisting}

Bij het gebruik van Vagrant, komen er ook een paar vaak voorkomende commando's bij kijken. Zo is 'vagrant up' het commando dat gebruikt wordt om alle virtuele machines aan te zetten. Wanneer er één server alleen moet worden aangezet, kan er ook specifiek een virtuele machine aangeroepen worden: 'vagrant up pgnode1'. Dit zal enkel de virtuele machine met als naam pgnode1 opstarten. Een volgend veelgebruikt commando is 'vagrant provision'. Dit commando zal worden gebruikt wanneer er in de configuratiebestanden wijzigingen zijn toegebracht. Dit laat toe om deze wijzigingen uit te voeren op alle virtuele machines. Ook hier kan er specifiek één virtuele machine aangeroepen worden, dit is net zoals hierboven, door een specifieke naam mee te geven als parameter. 'vagrant halt' zal alle virtuele machines uitzetten. 'vagrant ssh pgnode1' zal ssh-connectie maken met de meegegeven virtuele machine als parameter.

Na het opzetten van onze Vagrantfile wordt deze via 'vagrant up' geactiveerd in een command line. Dit kan in Windows Powershell, maar ook in een Linux terminal of een andere soort terminal.

Vagrant is niet standaard geïnstalleerd op een Ubuntu machine. Via 'sudo apt-get install vagrant' wordt Vagrant geïnstalleerd.

\subsection{\IfLanguageName{dutch}{VirtualBox}{VirtualBox}}
\label{subsec:VirtualBox}
In deze proof of concept zal gebruik gemaakt worden van virtuele machines via VirtualBox. VirtualBox is een open source virtualisatiesoftware. Het werkt als een hypervisor en kan op deze manier virtuele machines aanmaken, waarin de gebruiker naar eigen keuze verschillende besturingssytemen kan emuleren. Bij de configuratie van een virtuele machine kunnen de specificaties zoals RAM-geheugen en schijfruimte bepaald worden.

%Hier nog wat tekst bij.


\section{\IfLanguageName{dutch}{pgServer}{pgServer}}
\label{sec:pgServer}
Tijdens de configuratie van de cluster zal pgServer '172.16.0.11' als ip adres hebben.

Als eerste stap moeten de juiste packages geïnstalleerd worden op pgServer. Hier gaat het over Python, Consul, pgBouncer en HAProxy. Dit zal gebeuren aan de hand van het “postgresql-cluster-setup.sh” bestand die wordt aangeroepen in de Vagrantfile.

Hierna zal voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service wordt verwezen naar het config bestand voor de Consul server. Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "advertise_addr": "172.16.0.11",
        "bind_addr": "172.16.0.11",
        "bootstrap": false,
        "bootstrap_expect": 1,
        "server": true,
        "client_addr": "127.0.0.1",
        "node_name": "pgServer",
        "datacenter": "dc1",
        "data_dir": "/var/consul/server",
        "domain": "consul",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==",
        "log_level": "INFO",
        "enable_syslog": true,
        "rejoin_after_leave": true,
        "ui_dir": "/var/consul/ui",
        "leave_on_terminate": false,
        "skip_leave_on_interrupt": false
    }
    
 \end{lstlisting}

Het “advertise\_addr” of het adverteer adres wordt gebruikt om het IP-adres te wijzigen dat geadverteerd wordt naar de andere nodes. Standaard wordt voor advertising het “bind\_addr” geadverteerd.

De “bootstrap” parameter toont aan of de server in “bootstrap” modus staat.
“bootstrap\_expect” geeft het aantal verwachte servers mee in de cluster als parameter. Consul zal wachten tot dit aantal beschikbaar is om de cluster te bootstrappen.

Het “client\_addr” geeft het adres mee waaraan Consul client interfaces zal binden, zoals de HTTP en DNS servers. Standaard staat dit op “127.0.0.1”.

“datacenter” controleert het datacenter waarin de Consul agent draait. Standaard staat deze ingesteld op “dc1”.

Consul zal aan de hand van “rejoin\_after\_leave” er altijd voor proberen zorgen dat de node bij opstart de cluster zal proberen te joinen. Standaard beschouwd Consul een vertrek van een node uit de cluster, als permanent. Met de parameter hier op “true” te zetten, zal dit niet het geval zijn en zal de node proberen de cluster terug te joinen.

“leave\_on\_terminate” zal, indien “true”, wanneer de node een signaal krijgt om af te sluiten, de node de cluster verlaten. Voor Consul clients staat dit standaard op “true”, maar bij Consul servers staat dit standaard op “false”. “skip\_leave\_on\_interrupt” is zeer vergelijkbaar met “leave\_on\_terminate”.

%https://www.consul.io/docs/agent/options

Bij het installeren van de HAProxy wordt doorverwezen naar een configuratiebestand waarin de verschillende opties worden meegegeven.

\begin{lstlisting}
global
    maxconn 100

defaults
    log global
    mode tcp
    retries 2
    timeout client 30m
    timeout connect 4s
    timeout server 30m
    timeout check 5s

listen stats
    mode http
    bind *:7000
    stats enable
    stats uri /

listen postgres
    bind *:5000
    option httpchk
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server postgresql_pgNode1_172.0.16.22 172.16.0.22:5432 maxconn 100 check port 8008
    server postgresql_pgNode2_172.0.16.33 172.16.0.33:5432 maxconn 100 check port 8008
\end{lstlisting}

“maxconn” zal eeen limiet zetten op het aantal verbindingen dat HAProxy zal aanvaarden.

“mode” zal bepalen op welke manier HAProxy werkt. Aan de ene kant kan het werken als een eenvoudige TCP proxy. Aan de andere kant is het ook in staat om HTTP-berichten van binnenkomend verkeer te inspecteren.

De “timeout” commando's zullen na een bepaalde time-out van client of server de verbinding verbreken.

“bind” zal een listener toewijzen aan een bepaald IP-adres en poort.

“option httpchk” zal gezondheidscontroles uitvoeren op HTTP. Servers die hier niet op reageren, zullen niet meer bediend worden.

De “default-server” configureert voor alle serverregels de standaardinstellingen, zoals het activeren van de gezondheidscontroles en maximale verbindingen.

Bij “server” staat voor elke aanwezig node het IP-adres, de poort en het maximum aantal connecties.
%https://www.haproxy.com/blog/the-four-essential-sections-of-an-haproxy-configuration/

Bij het configureren van pgBouncer wordt gebruik gemaakt van een .ini file. Deze ziet er als volgt uit:

\begin{lstlisting}
[databases]
postgres = host=172.16.0.11 port=5000 pool_size=6
template1 = host=172.16.0.11 port=5000 pool_size=6
test = host=172.16.0.11 port=5000 pool_size=6

[pgbouncer]
logfile = /var/log/postgresql/pgbouncer.log
pidfile = /var/run/postgresql/pgbouncer.pid
listen_addr = *
listen_port = 6432
unix_socket_dir = /var/run/postgresql
auth_type = trust
auth_file = /etc/pgbouncer/userlist.txt
admin_users = postgres
stats_users =
pool_mode = transaction
server_reset_query =
server_check_query = select 1
server_check_delay = 10
max_client_conn = 1000
default_pool_size = 12
reserve_pool_size = 5
log_connections = 1
log_disconnections = 1
log_pooler_errors = 1  
\end{lstlisting}

%https://www.pgbouncer.org/config.html

“listen\_addr” specificeert het aantal IP-adressen waar naar geluisterd moet worden. Een * zal duiden dat naar alle adressen geluisterd zal worden. “listen\_port” zal de poort meegeven waarnaar geluisterd moet worden.

“auth\_type” zal bepalen hoe gebruikers geauthenticeerd worden. Bij de parameter trust zal er geen authenticatie plaatsvinden.

Bij “admin\_users” zal worden bepaald welke gebruikers volledige controle hebben.

“server\_check\_query” controleert of de server connection aanwezig is. “server\_check\_delay” zal meegeven hoelang een verbinding beschikbaar moet blijven.

“default\_pool\_size” en “reserve\_pool\_size” zal het (maximum) aantal pools bepalen.


\section{\IfLanguageName{dutch}{pgNode1}{pgNode1}}
\label{sec:pgNode1}
Tijdens de configuratie van de cluster zal pgNode1 '172.16.0.22' als ip adres hebben.

Idem als bij pgServer is de eerste stap het installeren van de juiste packages. Hier gaat het over Python, Consul Agent, Patroni en PostgreSQL uiteraard. Dit zal opnieuw gebeuren aan de hand van het “postgresql-cluster-setup.sh” bestand die wordt aangeroepen in de Vagrantfile. Deze file is te vinden onderaan dit hoofdstuk in 

Bij het configureren van Patroni zal verwezen worden naar pgNode1Patroni.yml waarin de configuratie te vinden is. Zoals eerder al vermeld is geweest, is de configuratie van Patroni te vinden in het DCS, het Distributed Configuration Store. Hierin staat onder andere de configuratie voor pg\_rewind en maximum\_lag\_on\_failover. Hieronder de configuratie:

\begin{lstlisting}
scope: PatroniCluster
namespace: /nsPatroniCluster
name: pgNode1

log:
    level: INFO
    dir: /var/log/patroni
    file_size: 10485760 #staat gelijk aan ongeveer 10MB

restapi:
    listen: 172.16.0.22:8008
    connect_address: 172.16.0.22:8008

consul:
    host: 172.16.0.11:8500

bootstrap:
    dcs:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576 #staat gelijk aan ongeveer 1MB
        master_start_timeout: 300
        postgresql:
            use_pg_rewind: true
            parameters:
                archive_command: 'exit 0'
                archive_mode: 'on'
                autovacuum: 'on'
                checkpoint_completion_target: 0.6
                checkpoint_warning: 300
                datestyle: 'iso, mdy'
                default_text_search_config: 'pg_catalog.english'
                effective_cache_size: '128MB'
                hot_standby: 'on'
                include_if_exists: 'repmgr_lib.conf'
                lc_messages: 'C'
                listen_addresses: '*'
                log_autovacuum_min_duration: 0
                log_checkpoints: 'on'
                logging_collector: 'on'
                log_min_messages: INFO
                log_filename: 'postgresql.log'
                log_connections: 'on'
                log_directory: '/var/log/postgresql'
                log_disconnections: 'on'
                log_line_prefix: '%t [%p]: [%l-1] user=%u,db=%d,app=%a '
                log_lock_waits: 'on'
                log_min_duration_statement: 0
                log_temp_files: 0
                maintenance_work_mem: '128MB'
                max_connections: 101
                max_wal_senders: 5
                port: 5432
                shared_buffers: '128MB'
                shared_preload_libraries: 'pg_stat_statements'
                unix_socket_directories: '/var/run/postgresql'
                wal_buffers: '8MB'
                wal_keep_segments: '200'
                wal_level: 'replica'
                work_mem: '128MB'

    initdb:
    - encoding: UTF8
    - data-checksums

    pg_hba:
    - host replication replicator 127.0.0.1/32 md5
    - host replication replicator 172.16.0.22/0 md5
    - host replication replicator 172.16.0.33/0 md5
    - host all postgres 172.16.0.11/32 trust
    - host all postgres 172.16.0.22/32 trust
    - host all postgres 172.16.0.33/32 trust
    - host all all 0.0.0.0/0 md5
    - local all all peer
    
    users:
        admin:
            password: admin
            options:
                - createrole
                - createdb

postgresql:
    listen: 127.0.0.1,172.16.0.22:5432
    connect_address: 172.16.0.22:5432
    data_dir: /var/lib/postgresql/patroni
    pgpass: /tmp/pgpass
    use_unix_socket: true
    authentication:
        replication:
            username: replication
            password: replication
        superuser:
            username: postgres
            password: postgres
        parameters:
            unix_socket_directories: '/var/run/postgresql'
            wal_compression: on

tags:
    nofailover: false
    noloadbalance: false
    clonefrom: false
    nosync: false

watchdog:
    mode: off
\end{lstlisting}

“ttl” geeft de tijd, in seconden, mee voor de primary node om het leader lock te verwerven. Wanneer deze ttl gepasseerd is, zal failover plaatsvinden. Standaard staat deze waarde op 30 seconden.

Bij “retry\_timeout” wordt de tijd meegegeven die na overschrijden, de primary node zal degraderen, in geval van time-out. Standaard staat deze waarde op 10 seconden.

“maximum\_lag\_on\_failover” geeft het maximaal aantal bytes mee dat een standby node mag vertragen (lag) om deel te nemen aan een verkiezing van nieuwe primary node, in geval van failover. 

Bij “master\_start\_timeout” wordt de tijd meegegeven waarop een primary node de tijd krijgt om te herstellen van een fout voordat failover ingeschakeld wordt. Standaard staat deze parameter op 300 seconden. Als deze parameter op 0 staat, zal direct failover worden gedaan.

“use\_pg\_rewind” zal hier op true staan, omdat deze optie gebruikt zal worden. Standaard staat dit op false.

Bij “parameters” is er een lijst terug te vinden van commando's die gebruikt worden om Postgres te configureren.

“pg\_hba” zorgt voor client authenticatie tussen de PostgreSQL server en de client applicatie.

Bij tags zal “nofailover” aan de hand van de meegegeven parameter kijken of de node mag deelnemen aan de verkiezing om nieuwe primary node te worden. “clonefrom” zal dienen, wanneer true, voor de andere nodes om deze node te gebruiken voor bootstrap.
“nosync” zal indien true nooit geselecteerd worden voor synchrone replica.



Hierna zal opnieuw voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service wordt verwezen naar het config bestand voor de Consul agent (cliënt). Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "server": false,
        "datacenter": "dc1",
        "data_dir": "/var/consul/client",
        "ui_dir": "/var/consul/ui",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==", 
        "log_level": "INFO",
        "enable_syslog": true,
        "start_join": ["172.16.0.11"],
        "bind_addr": "172.16.0.22"
    }  
\end{lstlisting}


\section{\IfLanguageName{dutch}{pgNode2}{pgNode2}}
\label{sec:pgNode2}
Tijdens de configuratie van de cluster zal pgNode2 '172.16.0.33' als ip adres hebben.

Idem als bij pgServer is de eerste stap het installeren van de juiste packages. Hier gaat het over Python, Consul Agent, Patroni en PostgreSQL uiteraard. Dit zal opnieuw gebeuren aan de hand van het “postgresql-cluster-setup.sh” bestand die wordt aangeroepen in de Vagrantfile.

Bij het configureren van Patroni zal verwezen worden naar pgNode2Patroni.yml waarin de configuratie te vinden is. Dit bestand is zeer gelijkend aan wat er bij pgNode1 staat, maar kent toch een paar kleine wijzigingen, maar enkel in ip-adressen:

\begin{lstlisting}
restapi:
    listen: 172.16.0.33:8008
    connect_address: 172.16.0.33:8008

consul:
    host: 172.16.0.11:8500
\end{lstlisting}

\begin{lstlisting}
postgresql:
    listen: 127.0.0.1,172.16.0.33:5432
    connect_address: 172.16.0.33:5432
    data_dir: /var/lib/postgresql/patroni
    pgpass: /tmp/pgpass
    use_unix_socket: true
    authentication:
        replication:
            username: replication
            password: replication
        superuser:
            username: postgres
            password: postgres
    parameters:
        unix_socket_directories: '/var/run/postgresql'
        wal_compression: on
\end{lstlisting}


\begin{lstlisting}
    
\end{lstlisting}


Hierna zal opnieuw voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service wordt verwezen naar het config bestand voor de Consul agent (cliënt). Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "server": false,
        "datacenter": "dc1",
        "data_dir": "/var/consul/client",
        "ui_dir": "/var/consul/ui",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==", 
        "log_level": "INFO",
        "enable_syslog": true,
        "start_join": ["172.16.0.11"],
        "bind_addr": "172.16.0.33"
    }
\end{lstlisting}

In onderstaande blok code staat het “postgresql-setup-cluster.sh” bestand die door Vagrant zal aangeroepen worden. Hierin zullen de twee nodes en server mee geconfigureerd worden.

\begin{lstlisting}  
    #!/bin/bash
    
    POSTGRESQL_VERSION=12
    PGBOUNCER_VERSION=1.15.0-1.pgdg16.04+1
    CONSUL_VERSION=1.9.5
    PATRONI_VERSION=2.0.2
    PSYCOPG2_VERSION=2.8.6
    PYCONSUL_VERSION=1.2.3
    
    PG_PATH="/etc/postgresql/${POSTGRESQL_VERSION}/main"
    
    function setup_packages() {
        apt-get update
        apt-get -y install wget unzip curl libpq-dev ca-certificates ntp tree
    }
    
    function setup_python() {
        #installl pip
        apt-get -y install python python-pip
    }
    
    function setup_patroni() {
        # Install
        pip install -U pip
        pip install psycopg2-binary==${PSYCOPG2_VERSION}
        pip install python-consul==${PYCONSUL_VERSION}
        pip install -U flake8 configparser zipp
        pip install patroni[consul]==${PATRONI_VERSION}
        mkdir -p /etc/patroni
        mkdir -p /var/lib/postgresql/patroni
        chmod 700 /var/lib/postgresql/patroni
        mkdir -p /var/log/patroni
        chmod 777 /var/log/patroni
        
        # Configure Patroni on nodes
        if [ "$(hostname)" == "pgNode1" ]; then
        cp -p /vagrant/pgNode1Patroni01.yml /etc/patroni/patroni.yml
        elif [ "$(hostname)" == "pgNode2" ]; then # consul server and client
        cp -p /vagrant/pgNode1Patroni02.yml /etc/patroni/patroni.yml
        fi
        cp -p /vagrant/patroni.service /etc/systemd/system/
        echo "export PATRONICTL_CONFIG_FILE=/etc/patroni/patroni.yml" >> /etc/environment
        source /etc/environment
        systemctl daemon-reload
    }
    
    function setup_consul() {
        # Install consul on 2 nodes en 1 server
        curl -s -O https://releases.hashicorp.com/consul/${CONSUL_VERSION}/consul_${CONSUL_VERSION}_linux_amd64.zip
        unzip consul_${CONSUL_VERSION}_linux_amd64.zip
        mv consul /usr/local/bin/
        rm -f consul_${CONSUL_VERSION}_linux_amd64.zip
        mkdir -p /etc/consul.d/{server,client}
        
        if [ "$(hostname)" == "pgNode1" ]; then
        cp -p /vagrant/consul.d/client/pgNode1.json /etc/consul.d/client/pgNode1.json
        fi
        if [ "$(hostname)" == "pgNode2" ]; then
        cp -p /vagrant/consul.d/client/pgNode2.json /etc/consul.d/client/pgNode2.json
        fi
        if [ "$(hostname)" == "pgServer" ]; then
        cp -rp /vagrant/consul.d/server/ /etc/consul.d
        fi
        mkdir -p /var/consul/ui
        useradd -M -d /var/consul -s /bin/bash consul
        
        # Configure consul service
        mkdir -p /var/consul/{server,client}
        if [ "$(hostname)" == "pgNode1" ]; then # consul client
        cp -p /vagrant/consul-client.service /etc/systemd/system/
        fi
        if [ "$(hostname)" == "pgNode2" ]; then # consul client
        cp -p /vagrant/consul-client.service /etc/systemd/system/
        fi
        if [ "$(hostname)" == "pgServer" ]; then # consul server
        cp -p /vagrant/consul-server.service /etc/systemd/system/
        fi
        systemctl daemon-reload
        
        chown -R consul:consul /var/consul/
        
        #start consul service
        if [ "$(hostname)" == "pgNode1" ]; then
        systemctl start consul-client
        systemctl enable consul-client
        fi
        if [ "$(hostname)" == "pgNode2" ]; then
        systemctl start consul-client
        systemctl enable consul-client
        fi
        if [ "$(hostname)" == "pgServer" ]; then
        systemctl start consul-server
        systemctl enable consul-server
        fi
    }
    
    function setup_haproxy() {
        # Install haproxy
        apt-get install haproxy -y
        
        # Configure
        cp -p /vagrant/haproxy.cfg /etc/haproxy/haproxy.cfg
        
        # start haproxy.service
        systemctl restart haproxy
        systemctl enable haproxy
    }
    
    function setup_pgbouncer() {
        # Install psycopg2 zodat python scripts kunnen runnen
        pip install psycopg2-binary==${PSYCOPG2_VERSION}
        
        # Install the version that comes with the official apt postgresql repository
        apt-get -y install pgbouncer=${PGBOUNCER_VERSION} postgresql-client-${POSTGRESQL_VERSION}
        
        cat > /etc/pgbouncer/pgbouncer.ini <<EOF
        [databases]
        postgres = host=172.16.0.11 port=5000 pool_size=6
        template1 = host=172.16.0.11 port=5000 pool_size=6
        test = host=172.16.0.11 port=5000 pool_size=6
        
        [pgbouncer]
        logfile = /var/log/postgresql/pgbouncer.log
        pidfile = /var/run/postgresql/pgbouncer.pid
        listen_addr = *
        listen_port = 6432
        unix_socket_dir = /var/run/postgresql
        auth_type = trust
        auth_file = /etc/pgbouncer/userlist.txt
        admin_users = postgres
        stats_users =
        pool_mode = transaction
        server_reset_query =
        server_check_query = select 1
        server_check_delay = 10
        max_client_conn = 1000
        default_pool_size = 12
        reserve_pool_size = 5
        log_connections = 1
        log_disconnections = 1
        log_pooler_errors = 1
        EOF
        
        cat > /etc/pgbouncer/userlist.txt <<EOF
        "postgres" "password"
        EOF
        
        cat > /etc/default/pgbouncer <<EOF
        START=1
        EOF
        systemctl restart pgbouncer
        systemctl enable pgbouncer
    }
    
    function setup_postgresql_repo() {
        # Setup postgresql repo
        echo "deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list
        
        # Setup postgresql repo key
        wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add -
        
        apt-get update
    }
    
    function setup_postgresql() {
        # Install postgresql
        apt-get -y install postgresql-${POSTGRESQL_VERSION}
        
        #Stop service
        systemctl stop postgresql
        systemctl disable postgresql
        
        chown -R postgres:postgres /var/lib/postgresql/patroni
        chown -R postgres:postgres /etc/patroni
        
        
        # Patroni binaries check
        ln -f -s /usr/lib/postgresql/${POSTGRESQL_VERSION}/bin/ * /usr/bin/
        
        # Start patroni.service
        systemctl start patroni
        systemctl enable patroni
    }
    
    # Timezone
    timedatectl set-timezone Europe/Brussels
    
    setup_packages
    
    if [ ! -f /usr/bin/pip ]; then
    setup_python
    fi
    
    if [ "$(hostname)" != "pgServer" ]; then
    if [ ! -f /usr/local/bin/patroni ]; then
    setup_patroni
    fi
    fi
    
    if [ ! -f /usr/local/bin/consul ]; then
    setup_consul
    fi
    
    # Install HAProxy in pgServer
    if [ "$(hostname)" == "pgServer" ]; then
    if [ ! -f /etc/haproxy/haproxy.cfg ]; then
    setup_haproxy
    fi
    fi
    
    if [ ! -f /etc/apt/sources.list.d/pgdg.list ]; then
    setup_postgresql_repo
    fi
    
    if [ "$(hostname)" != "pgServer" ]; then
    if [ ! -f ${PG_PATH}/postgresql.conf ]; then
    setup_postgresql
    fi
    fi
    
    if [ "$(hostname)" == "pgServer" ]; then
    if [ ! -f /etc/pgbouncer/pgbouncer.ini ]; then
    setup_pgbouncer
    fi
    fi
\end{lstlisting}

Deze proof of concept is gebaseerd op een cluster gemaakt door Vicenç Juan Tomàs Montserrat~\textcite{Montserrat2020}. Zijn opstelling is de basis geweest voor het opzetten van deze Patroni cluster. De opzet van deze cluster omvatten de drie functionaliteiten die nodig waren om te voldoen aan de functionele requirements, namelijk replicatie, monitoring en failover.
%\section{\IfLanguageName{dutch}{Testing functionele requirements}{Testing functionele requirements}}
%\label{sec:Testing functionele requirements}


\section{\IfLanguageName{dutch}{Persoonlijke conclusie}{Persoonlijke conclusie}}
\label{sec:Persoonlijke conclusie}

Het opzetten van de Patroni cluster verliep voor mij redelijk moeizaam. Het was een proces van trial and error. Het was niet direct duidelijk welke configuraties en tools er precies nodig waren voor de opzet van de cluster.
Wat mij wel direct opviel, is dat alles te configureren valt in de verschillende configuratiebestanden. De cluster kan naar believen geconfigureerd worden volgens de verschillende noden van gebruikers.


%Hierin persoonlijke ervaring over Patroni schrijven als oplossing. Is het aan te raden, is het een eenvoudige oplossing...
