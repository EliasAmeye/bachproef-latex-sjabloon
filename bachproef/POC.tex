%%=============================================================================
%% Conclusie
%%=============================================================================

\chapter{Proof of Concept}
\label{ch:Proof of Concept}

In dit hoofdstuk wordt de opbouw van de Patroni cluster uitgelegd. Hierin zal worden gekeken hoe de cluster voldoet aan de functionele requirements, namelijk redundantie/replicatie, failover en monitoring.

\section{\IfLanguageName{dutch}{Omgeving}{Environment}}
\label{sec:Omgeving}
De opzet van deze proof of concept zal gebeuren in een lokale Linux-omgeving, namelijk Ubuntu 21.04.
De cluster zal bestaan uit drie virtuele machines die zullen draaien op Ubuntu Xenial 16.04. De eerste virtuele machine (pgServer) is een servernode waarop Consul zal draaien, samen met pgBouncer en een HAProxy. De andere twee virtuele machines (pgNode1 en pgNode2) zullen PostgreSQL, Patroni en een Consul Agent draaien.

De cluster zal dus zo geconfigueerd worden dat er één primary node is met een standby node die asynchrone streaming replicatie verricht.

\section{\IfLanguageName{dutch}{Prerequisites}{Prerequisites}}
\label{sec:Prerequisites}

\subsection{\IfLanguageName{dutch}{Vagrant}{Vagrant}}
\label{subsec:Vagrant}
Bij de opzet van cluster zal gebruik worden gemaakt van Vagrant. Vagrant is een tool voor het bouwen en beheren van virtuele machine-omgevingen~\autocite{Kalow2020}.

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=Java,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

In onderstaande code snippet is de code voor de Vagrantfile zichtbaar. Hierin wordt bepaald welke servers aangemaakt worden en welke opties zij bezitten. Zo zal elke node een RAM-geheugen hebben van 1 GB (1024 MB). Ook het ip-adres zal hierin worden toegewezen per node. Er wordt ook een file meegegeven die zal worden uitgevoerd in de server. Deze file bevat de opbouw van de cluster, zoals het installeren van Consul, het configureren van Patroni op de nodes, en de setup van HAProxy en pgBouncer.

\begin{lstlisting}
# -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    config.vm.box = "ubuntu/xenial64"
    config.vm.provider "virtualbox" do |v|
        v.customize ["modifyvm", :id, "--memory", 1024]
        v.gui = true
    end

    config.vm.define :pgServer do |pgServer_config|
        pgServer_config.vm.hostname = 'pgServer'
        pgServer_config.vm.network :private_network, ip: "172.16.0.11"
        pgServer_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
    config.vm.define :pgNode1, primary: true do |pgNode1_config|
        pgNode1_config.vm.hostname = 'pgNode1'
        pgNode1_config.vm.network :private_network, ip: "172.16.0.22"
        pgNode1_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
    config.vm.define :pgNode2 do |pgNode2_config|
        pgNode2_config.vm.hostname = 'pgNode2'
        pgNode2_config.vm.network :private_network, ip: "172.16.0.33"
        pgNode2_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
end
\end{lstlisting}

\subsection{\IfLanguageName{dutch}{VirtualBox}{VirtualBox}}
\label{subsec:VirtualBox}
In deze proof of concept zal gebruik gemaakt worden van virtuele machines via VirtualBox. VirtualBox is een open source virtualisatiesoftware. Het werkt als een hypervisor en kan op deze manier virtuele machines aanmaken, waarin de gebruiker naar eigen keuze verschillende besturingssytemen kan emuleren. Bij de configuratie van een virtuele machine kunnen de specificaties zoals RAM-geheugen, schijfruimte, etc. bepaald worden.

%Hier nog wat tekst bij.


\section{\IfLanguageName{dutch}{pgServer}{pgServer}}
\label{sec:pgServer}
Tijdens de configuratie van de cluster zal pgServer '172.16.0.11' als ip adres hebben.

Als eerste stap moeten de juiste packages geïnstalleerd worden op pgServer. Hier gaat het over Python, Consul, pgBouncer en HAProxy.

Hierna zullen we voor Consul een service toevoegen die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service verwijzen we naar de config file voor de Consul server. Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "advertise_addr": "172.16.0.11",
        "bind_addr": "172.16.0.11",
        "bootstrap": false,
        "bootstrap_expect": 1,
        "server": true,
        "client_addr": "0.0.0.0",
        "node_name": "pgServer",
        "datacenter": "dc1",
        "data_dir": "/var/consul/server",
        "domain": "consul",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==",
        "log_level": "INFO",
        "enable_syslog": true,
        "rejoin_after_leave": true,
        "ui_dir": "/var/consul/ui",
        "leave_on_terminate": false,
        "skip_leave_on_interrupt": false
    }
    
 \end{lstlisting}

Bij het installeren van de HAProxy wordt doorverwezen naar een configuratiefile waarin de verschillende opties worden meegegeven.

\begin{lstlisting}
global
    maxconn 100

defaults
    log global
    mode tcp
    retries 2
    timeout client 30m
    timeout connect 4s
    timeout server 30m
    timeout check 5s

listen stats
    mode http
    bind *:7000
    stats enable
    stats uri /

listen postgres
    bind *:5000
    option httpchk
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server postgresql_pgNode1_172.0.16.22 172.16.0.22:5432 maxconn 100 check port 8008
    server postgresql_pgNode2_172.0.16.33 172.16.0.33:5432 maxconn 100 check port 8008
\end{lstlisting}

De pgBouncer.ini file ziet er als volgt uit:

\begin{lstlisting}
[databases]
postgres = host=172.16.0.11 port=5000 pool_size=6
template1 = host=172.16.0.11 port=5000 pool_size=6
test = host=172.16.0.11 port=5000 pool_size=6

[pgbouncer]
logfile = /var/log/postgresql/pgbouncer.log
pidfile = /var/run/postgresql/pgbouncer.pid
listen_addr = *
listen_port = 6432
unix_socket_dir = /var/run/postgresql
auth_type = trust
auth_file = /etc/pgbouncer/userlist.txt
admin_users = postgres
stats_users =
pool_mode = transaction
server_reset_query =
server_check_query = select 1
server_check_delay = 10
max_client_conn = 1000
default_pool_size = 12
reserve_pool_size = 5
log_connections = 1
log_disconnections = 1
log_pooler_errors = 1  
\end{lstlisting}


\section{\IfLanguageName{dutch}{pgNode1}{pgNode1}}
\label{sec:pgNode1}
Tijdens de configuratie van de cluster zal pgNode1 '172.16.0.22' als ip adres hebben.


Idem als bij pgServer is de eerste stap het installeren van de juiste packages. Hier gaat het over Python, Consul Agent, Patroni en PostgreSQL uiteraard.

Bij het configureren van Patroni zal verwezen worden naar pgNode1Patroni.yml waarin de configuratie te vinden is. Zoals eerder al vermeld is geweest, is de configuratie van Patroni te vinden in het DCS, het Distributed Configuration Store. Hierin staat onder andere de configuratie voor pg\_rewind, maximum\_lag\_on\_failover, etc.:

\begin{lstlisting}
scope: PatroniCluster
namespace: /nsPatroniCluster
name: pgNode1

log:
    level: INFO
    dir: /var/log/patroni
    file_size: 10485760 #staat gelijk aan ongeveer 10MB

restapi:
    listen: 172.16.0.22:8008
    connect_address: 172.16.0.22:8008

consul:
    host: 172.16.0.11:8500

bootstrap:
    dcs:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576 #staat gelijk aan ongeveer 1MB
        master_start_timeout: 300
        postgresql:
            use_pg_rewind: true
            parameters:
                archive_command: 'exit 0'
                archive_mode: 'on'
                autovacuum: 'on'
                checkpoint_completion_target: 0.6
                checkpoint_warning: 300
                datestyle: 'iso, mdy'
                default_text_search_config: 'pg_catalog.english'
                effective_cache_size: '128MB'
                hot_standby: 'on'
                include_if_exists: 'repmgr_lib.conf'
                lc_messages: 'C'
                listen_addresses: '*'
                log_autovacuum_min_duration: 0
                log_checkpoints: 'on'
                logging_collector: 'on'
                log_min_messages: INFO
                log_filename: 'postgresql.log'
                log_connections: 'on'
                log_directory: '/var/log/postgresql'
                log_disconnections: 'on'
                log_line_prefix: '%t [%p]: [%l-1] user=%u,db=%d,app=%a '
                log_lock_waits: 'on'
                log_min_duration_statement: 0
                log_temp_files: 0
                maintenance_work_mem: '128MB'
                max_connections: 101
                max_wal_senders: 5
                port: 5432
                shared_buffers: '128MB'
                shared_preload_libraries: 'pg_stat_statements'
                unix_socket_directories: '/var/run/postgresql'
                wal_buffers: '8MB'
                wal_keep_segments: '200'
                wal_level: 'replica'
                work_mem: '128MB'

    initdb:
    - encoding: UTF8
    - data-checksums

    pg_hba:
    - host replication replicator 127.0.0.1/32 md5
    - host replication replicator 172.16.0.22/0 md5
    - host replication replicator 172.16.0.33/0 md5
    - host all postgres 172.16.0.11/32 trust
    - host all postgres 172.16.0.22/32 trust
    - host all postgres 172.16.0.33/32 trust
    - host all all 0.0.0.0/0 md5
    - local all all peer
    
    users:
        admin:
            password: admin
            options:
                - createrole
                - createdb

postgresql:
    listen: 127.0.0.1,172.16.0.22:5432
    connect_address: 172.16.0.22:5432
    data_dir: /var/lib/postgresql/patroni
    pgpass: /tmp/pgpass
    use_unix_socket: true
    authentication:
        replication:
            username: replication
            password: replication
        superuser:
            username: postgres
            password: postgres
        parameters:
            unix_socket_directories: '/var/run/postgresql'
            wal_compression: on

tags:
    nofailover: false
    noloadbalance: false
    clonefrom: false
    nosync: false

watchdog:
    mode: off
\end{lstlisting}

Hierna zal opnieuw voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service verwijzen we naar de config file voor de Consul agent (cliënt). Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "server": false,
        "datacenter": "dc1",
        "data_dir": "/var/consul/client",
        "ui_dir": "/var/consul/ui",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==", 
        "log_level": "INFO",
        "enable_syslog": true,
        "start_join": ["172.16.0.11"],
        "bind_addr": "172.16.0.22"
    }  
\end{lstlisting}


\section{\IfLanguageName{dutch}{pgNode2}{pgNode2}}
\label{sec:pgNode2}
Tijdens de configuratie van de cluster zal pgNode2 '172.16.0.33' als ip adres hebben.

Idem als bij pgServer is de eerste stap het installeren van de juiste packages. Hier gaat het over Python, Consul Agent, Patroni en PostgreSQL uiteraard.

Bij het configureren van Patroni zal verwezen worden naar pgNode2Patroni.yml waarin de configuratie te vinden is. Dit bestand is zeer gelijkend aan wat er bij pgNode1 staat, maar kent toch een paar kleine wijzigingen, maar enkel in ip-adressen:

\begin{lstlisting}
restapi:
    listen: 172.16.0.33:8008
    connect_address: 172.16.0.33:8008

consul:
    host: 172.16.0.11:8500
\end{lstlisting}

\begin{lstlisting}
postgresql:
    listen: 127.0.0.1,172.16.0.33:5432
    connect_address: 172.16.0.33:5432
    data_dir: /var/lib/postgresql/patroni
    pgpass: /tmp/pgpass
    use_unix_socket: true
    authentication:
        replication:
            username: replication
            password: replication
        superuser:
            username: postgres
            password: postgres
    parameters:
        unix_socket_directories: '/var/run/postgresql'
        wal_compression: on
\end{lstlisting}


\begin{lstlisting}
    
\end{lstlisting}




Hierna zal opnieuw voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service verwijzen we naar de config file voor de Consul agent (cliënt). Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "server": false,
        "datacenter": "dc1",
        "data_dir": "/var/consul/client",
        "ui_dir": "/var/consul/ui",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==", 
        "log_level": "INFO",
        "enable_syslog": true,
        "start_join": ["172.16.0.11"],
        "bind_addr": "172.16.0.33"
    }
\end{lstlisting}

\begin{table}
\begin{tabular}{ |p{6cm}||p{6cm}|  }
    \hline
    \multicolumn{2}{|c|}{Ip-adressen} \\
    \hline
    pgServer & 172.16.0.11 \\
    \hline
    pgNode1 & 172.16.0.22 \\
    \hline
    pgNode2 & 172.16.0.33 \\
    \hline
\end{tabular}
\caption{Ip-adressen Cluster}
\label{table:Ip-adressen Cluster}
\end{table}


%\section{\IfLanguageName{dutch}{Testing functionele requirements}{Testing functionele requirements}}
%\label{sec:Testing functionele requirements}


\section{\IfLanguageName{dutch}{Persoonlijke conclusie}{Persoonlijke conclusie}}
\label{sec:Persoonlijke conclusie}

%Hierin persoonlijke ervaring over Patroni schrijven als oplossing. Is het aan te raden, is het een eenvoudige oplossing...

