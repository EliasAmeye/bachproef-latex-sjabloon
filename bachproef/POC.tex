%%=============================================================================
%% Conclusie
%%=============================================================================

\chapter{Proof of Concept}
\label{ch:Proof of Concept}

In dit hoofdstuk wordt de opbouw van de Patroni cluster uitgelegd. Hierin zal worden gekeken hoe de cluster voldoet aan de functionele requirements, namelijk replicatie, failover en monitoring.

\section{\IfLanguageName{dutch}{Omgeving}{Environment}}
\label{sec:Omgeving}
De opzet van deze proof of concept zal gebeuren in een lokale Linux-omgeving, namelijk Ubuntu 21.04.
De cluster zal bestaan uit drie virtuele machines die zullen draaien op Ubuntu Xenial 16.04. De eerste virtuele machine (pgServer) is een servernode waarop Consul zal draaien, samen met pgBouncer en een HAProxy. De andere twee virtuele machines (pgNode1 en pgNode2) zullen PostgreSQL, Patroni en een Consul Agent draaien.

De cluster zal dus zo geconfigueerd worden dat er één primary node is met een standby node die asynchrone streaming replicatie verricht.

\begin{table}
    \centering
    \begin{tabular}{ |p{6cm}||p{6cm}|  }
        \hline
        \multicolumn{2}{|c|}{Ip-adressen} \\
        \hline
        pgServer & 172.16.0.11 \\
        \hline
        pgNode1 & 172.16.0.22 \\
        \hline
        pgNode2 & 172.16.0.33 \\
        \hline
    \end{tabular}
    \caption{Ip-adressen Cluster}
    \label{table:Ip-adressen Cluster}
\end{table}

\section{\IfLanguageName{dutch}{Prerequisites}{Prerequisites}}
\label{sec:Prerequisites}

\subsection{\IfLanguageName{dutch}{Vagrant}{Vagrant}}
\label{subsec:Vagrant}
Bij de opzet van cluster zal gebruik worden gemaakt van Vagrant. Vagrant is een tool voor het bouwen en beheren van virtuele machine-omgevingen~\autocite{Kalow2020}.

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=Java,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

In onderstaande code snippet is de code voor de Vagrantfile zichtbaar. Hierin wordt bepaald welke servers aangemaakt worden en welke opties zij bezitten. Zo zal elke node een RAM-geheugen hebben van 1 GB (1024 MB). Ook het ip-adres zal hierin worden toegewezen per node. Er wordt ook een bestand meegegeven dat zal worden uitgevoerd in de server. Dit bestand bevat de opbouw van de cluster, zoals het installeren van Consul, het configureren van Patroni op de nodes, en de setup van HAProxy en pgBouncer.

Bij de drie virtuele machines wordt telkens verwezen naar een Linux bash-script, namelijk “postgresql-cluster-setup.sh”. Dit is het bestand waarin wordt gekeken met welke virtuele machine er gewerkt wordt, zodat, op basis daarvan, de juiste packages geïnstalleerd kunnen worden. Zo zal bijvoorbeeld bij pgNode1 Python, Consul Agent, Patroni en PostgreSQL geïnstalleerd worden als packages.

\begin{lstlisting}
# -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    config.vm.box = "ubuntu/xenial64"
    config.vm.provider "virtualbox" do |v|
        v.customize ["modifyvm", :id, "--memory", 1024]
        v.gui = true
    end

    config.vm.define :pgServer do |pgServer_config|
        pgServer_config.vm.hostname = 'pgServer'
        pgServer_config.vm.network :private_network, ip: "172.16.0.11"
        pgServer_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
    config.vm.define :pgNode1, primary: true do |pgNode1_config|
        pgNode1_config.vm.hostname = 'pgNode1'
        pgNode1_config.vm.network :private_network, ip: "172.16.0.22"
        pgNode1_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
    config.vm.define :pgNode2 do |pgNode2_config|
        pgNode2_config.vm.hostname = 'pgNode2'
        pgNode2_config.vm.network :private_network, ip: "172.16.0.33"
        pgNode2_config.vm.provision :shell, :path => "postgresql-cluster-setup.sh"
    end
end
\end{lstlisting}

Bij het gebruik van Vagrant, komen er ook een paar vaak voorkomende commando's bij kijken. Zo is 'vagrant up' het commando dat gebruikt wordt om alle virtuele machines aan te zetten. Wanneer er één server alleen moet worden aangezet, kan er ook specifiek een virtuele machine aangeroepen worden: 'vagrant up pgnode1'. Dit zal enkel de virtuele machine met als naam pgnode1 opstarten. Een volgend veelgebruikt commando is 'vagrant provision'. Dit commando zal worden gebruikt wanneer er in de configuratiebestanden wijzigingen zijn toegebracht. Dit laat toe om deze wijzigingen uit te voeren op alle virtuele machines. Ook hier kan er specifiek één virtuele machine aangeroepen worden, dit is net zoals hierboven, door een specifieke naam mee te geven als parameter. 'vagrant halt' zal alle virtuele machines uitzetten. 'vagrant ssh pgnode1' zal ssh-connectie maken met de meegegeven virtuele machine als parameter.

Na het opzetten van onze Vagrantfile wordt deze via 'vagrant up' geactiveerd in een command line. Dit kan in Windows Powershell, maar ook in een Linux terminal of een andere soort terminal.

Vagrant is niet standaard geïnstalleerd op een Ubuntu machine. Via 'sudo apt-get install vagrant' wordt Vagrant geïnstalleerd.

\subsection{\IfLanguageName{dutch}{VirtualBox}{VirtualBox}}
\label{subsec:VirtualBox}
In deze proof of concept zal gebruik gemaakt worden van virtuele machines via VirtualBox. VirtualBox is een open source virtualisatiesoftware. Het werkt als een hypervisor en kan op deze manier virtuele machines aanmaken, waarin de gebruiker naar eigen keuze verschillende besturingssytemen kan emuleren. Bij de configuratie van een virtuele machine kunnen de specificaties zoals RAM-geheugen en schijfruimte bepaald worden.

%Hier nog wat tekst bij.


\section{\IfLanguageName{dutch}{pgServer}{pgServer}}
\label{sec:pgServer}
Tijdens de configuratie van de cluster zal pgServer '172.16.0.11' als ip adres hebben.

Als eerste stap moeten de juiste packages geïnstalleerd worden op pgServer. Hier gaat het over Python, Consul, pgBouncer en HAProxy.

Hierna zal voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service wordt verwezen naar het config bestand voor de Consul server. Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "advertise_addr": "172.16.0.11",
        "bind_addr": "172.16.0.11",
        "bootstrap": false,
        "bootstrap_expect": 1,
        "server": true,
        "client_addr": "127.0.0.1",
        "node_name": "pgServer",
        "datacenter": "dc1",
        "data_dir": "/var/consul/server",
        "domain": "consul",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==",
        "log_level": "INFO",
        "enable_syslog": true,
        "rejoin_after_leave": true,
        "ui_dir": "/var/consul/ui",
        "leave_on_terminate": false,
        "skip_leave_on_interrupt": false
    }
    
 \end{lstlisting}

Het “advertise\_addr” of het adverteer adres wordt gebruikt om het IP-adres te wijzigen dat geadverteerd wordt naar de andere nodes. Standaard wordt voor advertising het “bind\_addr” geadverteerd.

De “bootstrap” parameter toont aan of de server in “bootstrap” modus staat.
“bootstrap\_expect” geeft het aantal verwachte servers mee in de cluster als parameter. Consul zal wachten tot dit aantal beschikbaar is om de cluster te bootstrappen.

Het “client\_addr” geeft het adres mee waaraan Consul client interfaces zal binden, zoals de HTTP en DNS servers. Standaard staat dit op “127.0.0.1”.

“datacenter” controleert het datacenter waarin de Consul agent draait. Standaard staat deze ingesteld op “dc1”.

Consul zal aan de hand van “rejoin\_after\_leave” er altijd voor proberen zorgen dat de node bij opstart de cluster zal proberen te joinen. Standaard beschouwd Consul een vertrek van een node uit de cluster, als permanent. Met de parameter hier op “true” te zetten, zal dit niet het geval zijn en zal de node proberen de cluster terug te joinen.

“leave\_on\_terminate” zal, indien “true”, wanneer de node een signaal krijgt om af te sluiten, de node de cluster verlaten. Voor Consul clients staat dit standaard op “true”, maar bij Consul servers staat dit standaard op “false”. “skip\_leave\_on\_interrupt” is zeer vergelijkbaar met “leave\_on\_terminate”.

%https://www.consul.io/docs/agent/options

Bij het installeren van de HAProxy wordt doorverwezen naar een configuratiebestand waarin de verschillende opties worden meegegeven.

\begin{lstlisting}
global
    maxconn 100

defaults
    log global
    mode tcp
    retries 2
    timeout client 30m
    timeout connect 4s
    timeout server 30m
    timeout check 5s

listen stats
    mode http
    bind *:7000
    stats enable
    stats uri /

listen postgres
    bind *:5000
    option httpchk
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server postgresql_pgNode1_172.0.16.22 172.16.0.22:5432 maxconn 100 check port 8008
    server postgresql_pgNode2_172.0.16.33 172.16.0.33:5432 maxconn 100 check port 8008
\end{lstlisting}

“maxconn” zal eeen limiet zetten op het aantal verbindingen dat HAProxy zal aanvaarden.

“mode” zal bepalen op welke manier HAProxy werkt. Aan de ene kant kan het werken als een eenvoudige TCP proxy. Aan de andere kant is het ook in staat om HTTP-berichten van binnenkomend verkeer te inspecteren.

De “timeout” commando's zullen na een bepaalde time-out van client of server de verbinding verbreken.

“bind” zal een listener toewijzen aan een bepaald IP-adres en poort.

“option httpchk” zal gezondheidscontroles uitvoeren op HTTP. Servers die hier niet op reageren, zullen niet meer bediend worden.

De “default-server” configureert voor alle serverregels de standaardinstellingen, zoals het activeren van de gezondheidscontroles en maximale verbindingen.

Bij “server” staat voor elke aanwezig node het IP-adres, de poort en het maximum aantal connecties.
%https://www.haproxy.com/blog/the-four-essential-sections-of-an-haproxy-configuration/



Het pgBouncer.ini bestand ziet er als volgt uit:

\begin{lstlisting}
[databases]
postgres = host=172.16.0.11 port=5000 pool_size=6
template1 = host=172.16.0.11 port=5000 pool_size=6
test = host=172.16.0.11 port=5000 pool_size=6

[pgbouncer]
logfile = /var/log/postgresql/pgbouncer.log
pidfile = /var/run/postgresql/pgbouncer.pid
listen_addr = *
listen_port = 6432
unix_socket_dir = /var/run/postgresql
auth_type = trust
auth_file = /etc/pgbouncer/userlist.txt
admin_users = postgres
stats_users =
pool_mode = transaction
server_reset_query =
server_check_query = select 1
server_check_delay = 10
max_client_conn = 1000
default_pool_size = 12
reserve_pool_size = 5
log_connections = 1
log_disconnections = 1
log_pooler_errors = 1  
\end{lstlisting}

%https://www.pgbouncer.org/config.html

“listen\_addr” specificeert het aantal IP-adressen waar naar geluisterd moet worden. Een * zal duiden dat naar alle adressen geluisterd zal worden. “listen\_port” zal de poort meegeven waarnaar geluisterd moet worden.

“auth\_type” zal bepalen hoe gebruikers geauthenticeerd worden. Bij de parameter trust zal er geen authenticatie plaatsvinden.

Bij “admin\_users” zal worden bepaald welke gebruikers volledige controle hebben.

“server\_check\_query” controleert of de server connection aanwezig is. “server\_check\_delay” zal meegeven hoelang een verbinding beschikbaar moet blijven.

“default\_pool\_size” en “reserve\_pool\_size” zal het (maximum) aantal pools bepalen.


\section{\IfLanguageName{dutch}{pgNode1}{pgNode1}}
\label{sec:pgNode1}
Tijdens de configuratie van de cluster zal pgNode1 '172.16.0.22' als ip adres hebben.


Idem als bij pgServer is de eerste stap het installeren van de juiste packages. Hier gaat het over Python, Consul Agent, Patroni en PostgreSQL uiteraard.

Bij het configureren van Patroni zal verwezen worden naar pgNode1Patroni.yml waarin de configuratie te vinden is. Zoals eerder al vermeld is geweest, is de configuratie van Patroni te vinden in het DCS, het Distributed Configuration Store. Hierin staat onder andere de configuratie voor pg\_rewind en  maximum\_lag\_on\_failover.

\begin{lstlisting}
scope: PatroniCluster
namespace: /nsPatroniCluster
name: pgNode1

log:
    level: INFO
    dir: /var/log/patroni
    file_size: 10485760 #staat gelijk aan ongeveer 10MB

restapi:
    listen: 172.16.0.22:8008
    connect_address: 172.16.0.22:8008

consul:
    host: 172.16.0.11:8500

bootstrap:
    dcs:
        ttl: 30
        loop_wait: 10
        retry_timeout: 10
        maximum_lag_on_failover: 1048576 #staat gelijk aan ongeveer 1MB
        master_start_timeout: 300
        postgresql:
            use_pg_rewind: true
            parameters:
                archive_command: 'exit 0'
                archive_mode: 'on'
                autovacuum: 'on'
                checkpoint_completion_target: 0.6
                checkpoint_warning: 300
                datestyle: 'iso, mdy'
                default_text_search_config: 'pg_catalog.english'
                effective_cache_size: '128MB'
                hot_standby: 'on'
                include_if_exists: 'repmgr_lib.conf'
                lc_messages: 'C'
                listen_addresses: '*'
                log_autovacuum_min_duration: 0
                log_checkpoints: 'on'
                logging_collector: 'on'
                log_min_messages: INFO
                log_filename: 'postgresql.log'
                log_connections: 'on'
                log_directory: '/var/log/postgresql'
                log_disconnections: 'on'
                log_line_prefix: '%t [%p]: [%l-1] user=%u,db=%d,app=%a '
                log_lock_waits: 'on'
                log_min_duration_statement: 0
                log_temp_files: 0
                maintenance_work_mem: '128MB'
                max_connections: 101
                max_wal_senders: 5
                port: 5432
                shared_buffers: '128MB'
                shared_preload_libraries: 'pg_stat_statements'
                unix_socket_directories: '/var/run/postgresql'
                wal_buffers: '8MB'
                wal_keep_segments: '200'
                wal_level: 'replica'
                work_mem: '128MB'

    initdb:
    - encoding: UTF8
    - data-checksums

    pg_hba:
    - host replication replicator 127.0.0.1/32 md5
    - host replication replicator 172.16.0.22/0 md5
    - host replication replicator 172.16.0.33/0 md5
    - host all postgres 172.16.0.11/32 trust
    - host all postgres 172.16.0.22/32 trust
    - host all postgres 172.16.0.33/32 trust
    - host all all 0.0.0.0/0 md5
    - local all all peer
    
    users:
        admin:
            password: admin
            options:
                - createrole
                - createdb

postgresql:
    listen: 127.0.0.1,172.16.0.22:5432
    connect_address: 172.16.0.22:5432
    data_dir: /var/lib/postgresql/patroni
    pgpass: /tmp/pgpass
    use_unix_socket: true
    authentication:
        replication:
            username: replication
            password: replication
        superuser:
            username: postgres
            password: postgres
        parameters:
            unix_socket_directories: '/var/run/postgresql'
            wal_compression: on

tags:
    nofailover: false
    noloadbalance: false
    clonefrom: false
    nosync: false

watchdog:
    mode: off
\end{lstlisting}

“ttl” geeft de tijd, in seconden, mee voor de primary node om het leader lock te verwerven. Wanneer deze ttl gepasseerd is, zal failover plaatsvinden. Standaard staat deze waarde op 30 seconden.

Bij “retry\_timeout” wordt de tijd meegegeven die na overschrijden, de primary node zal degraderen, in geval van time-out. Standaard staat deze waarde op 10 seconden.

“maximum\_lag\_on\_failover” geeft het maximaal aantal bytes mee dat een standby node mag vertragen (lag) om deel te nemen aan een verkiezing van nieuwe primary node, in geval van failover. 

Bij “master\_start\_timeout” wordt de tijd meegegeven waarop een primary node de tijd krijgt om te herstellen van een fout voordat failover ingeschakeld wordt. Standaard staat deze parameter op 300 seconden. Als deze parameter op 0 staat, zal direct failover worden gedaan.

“use\_pg\_rewind” zal hier op true staan, omdat we dit ingeschakeld willen. Standaard staat dit op false.

Bij “parameters” vinden we een lijst terug van commando's die gebruikt worden om Postgres te configureren.

“pg\_hba” zorgt voor client authenticatie tussen de PostgreSQL server en de client applicatie.

Bij tags zal “nofailover” aan de hand van de meegegeven parameter kijken of de node mag deelnemen aan de verkiezing om nieuwe primary node te worden. “clonefrom” zal dienen, wanneer true, voor de andere nodes om deze node te gebruiken voor bootstrap.
“nosync” zal indien true nooit geselecteerd worden voor synchrone replica.



Hierna zal opnieuw voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service wordt verwezen naar het config bestand voor de Consul agent (cliënt). Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "server": false,
        "datacenter": "dc1",
        "data_dir": "/var/consul/client",
        "ui_dir": "/var/consul/ui",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==", 
        "log_level": "INFO",
        "enable_syslog": true,
        "start_join": ["172.16.0.11"],
        "bind_addr": "172.16.0.22"
    }  
\end{lstlisting}


\section{\IfLanguageName{dutch}{pgNode2}{pgNode2}}
\label{sec:pgNode2}
Tijdens de configuratie van de cluster zal pgNode2 '172.16.0.33' als ip adres hebben.

Idem als bij pgServer is de eerste stap het installeren van de juiste packages. Hier gaat het over Python, Consul Agent, Patroni en PostgreSQL uiteraard.

Bij het configureren van Patroni zal verwezen worden naar pgNode2Patroni.yml waarin de configuratie te vinden is. Dit bestand is zeer gelijkend aan wat er bij pgNode1 staat, maar kent toch een paar kleine wijzigingen, maar enkel in ip-adressen:

\begin{lstlisting}
restapi:
    listen: 172.16.0.33:8008
    connect_address: 172.16.0.33:8008

consul:
    host: 172.16.0.11:8500
\end{lstlisting}

\begin{lstlisting}
postgresql:
    listen: 127.0.0.1,172.16.0.33:5432
    connect_address: 172.16.0.33:5432
    data_dir: /var/lib/postgresql/patroni
    pgpass: /tmp/pgpass
    use_unix_socket: true
    authentication:
        replication:
            username: replication
            password: replication
        superuser:
            username: postgres
            password: postgres
    parameters:
        unix_socket_directories: '/var/run/postgresql'
        wal_compression: on
\end{lstlisting}


\begin{lstlisting}
    
\end{lstlisting}


Hierna zal opnieuw voor Consul een service toegevoegd worden die bij het opstarten van de server wordt uitgevoerd. Belangrijk is dat bij het aanmaken van deze service, de service gestart en ge-enabled moet zijn. In deze service wordt verwezen naar het config bestand voor de Consul agent (cliënt). Deze ziet er als volgt uit.

\begin{lstlisting}
    {
        "server": false,
        "datacenter": "dc1",
        "data_dir": "/var/consul/client",
        "ui_dir": "/var/consul/ui",
        "encrypt": "/q/vkVS+My2nl8Zk/8csuQ==", 
        "log_level": "INFO",
        "enable_syslog": true,
        "start_join": ["172.16.0.11"],
        "bind_addr": "172.16.0.33"
    }
\end{lstlisting}



Deze proof of concept is gebaseerd op een cluster gemaakt door Vicenç Juan Tomàs Montserrat. Zijn opstelling is de basis geweest voor het opzetten van deze Patroni cluster. De opzet van deze cluster omvatten de drie functionaliteiten die nodig waren om te voldoen aan de functionele requirements, namelijk replicatie, monitoring en failover.
%\section{\IfLanguageName{dutch}{Testing functionele requirements}{Testing functionele requirements}}
%\label{sec:Testing functionele requirements}

%De cluster is op drie zaken gecontroleerd geweest:
%Is er replicatie aanwezig?

%Is er failover aanwezig?

%Is er monitoring aanwezig?


\section{\IfLanguageName{dutch}{Persoonlijke conclusie}{Persoonlijke conclusie}}
\label{sec:Persoonlijke conclusie}

Het opzetten van de Patroni cluster verliep voor mij redelijk moeizaam. Het is de eerste keer dat ik een cluster opgezet heb zonder te weten wat er concreet allemaal in moet zitten. Het was een proces van trial and error.


%Hierin persoonlijke ervaring over Patroni schrijven als oplossing. Is het aan te raden, is het een eenvoudige oplossing...

